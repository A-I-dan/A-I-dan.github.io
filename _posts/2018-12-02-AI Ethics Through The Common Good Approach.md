Aidan Wilson
Mrs. Ruch
Money Power and Social Justice
27  Nov 2018
Artificial Intelligence (AI) Through The Common Good Approach

In 2016, the computer program, AlphaGo, beat an 18 time world champion Go player, Lee Sedol, in a 5 game match. AlphaGo used deep neural networks to enable it to be trained by supervised learning from human moves as well as reinforcement learning by self-play. Shortly after, DeepMind introduced AlphaGo Zero, a new algorithm using reinforcement learning. This algorithm is given only the game rules with no data from human playerâ€™s moves. It essentially teaches itself to play the game based on predictions and self-play. AlphaGo Zero beat its predecessor program 100-0. While this Go playing program is only narrow AI, it shows the potential of how rapidly things can progress in the AI field. With the increased implementation of artificial intelligence across many disciplines, a new set of ethical issues is presenting itself. Ethical issues within AI include existential risk, superintelligence and the singularity, machine bias, weaponry, wealth distribution, unemployment, medical uses, and privacy. These issues have the potential to cause a lot of damage, but these damages may also be preventable. If researched and developed safely, AI could be the greatest tool ever created. The benefits of a symbiotic relationship with AI could far outweigh the disadvantages by enhancing humans and contributing to creating a greater society. If not researched and developed safely, AI could be the tool that destroys us. The ethical question that must be addressed is whether a strong push towards advancement in AI would benefit the community as a whole, and not just some elite members. With the risks in mind, further advancement in AI will not benefit the community as a whole, unless carefully monitored with measures taken to prevent the possible existential and social risks.
In 1950, Alan Turing developed the Turing test, an experiment to judge a machines ability to display and behave in an intelligent manner equal to a human. In the same year, Isaac Asimov published his book I, robot, which contained his three laws of robotics. (1) A robot may not injure a human being or, through inaction, allow a human being to come to harm. (2) A robot must obey the orders given it by human beings except where such orders would conflict with the First Law. (3) A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws. In 1951, Marvin Minsky and Dean Edmonds built the first artificial neural network, called SNARC, that successfully simulated a rats brain solving a maze. 
